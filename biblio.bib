
@article{YOKOTA2013445,
title = "Petascale turbulence simulation using a highly parallel fast multipole method on GPUs",
journal = "Computer Physics Communications",
volume = "184",
number = "3",
pages = "445 - 455",
year = "2013",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2012.09.011",
url = "http://www.sciencedirect.com/science/article/pii/S0010465512002974",
author = "Rio Yokota and L.A. Barba and Tetsu Narumi and Kenji Yasuoka",
keywords = "Isotropic turbulence, Fast multipole method, Integral equations, "
}

@inproceedings{Lashuk:2009:MPA:1654059.1654118,
 author = {Lashuk, Ilya and Chandramowlishwaran, Aparna and Langston, Harper and Nguyen, Tuan-Anh and Sampath, Rahul and Shringarpure, Aashay and Vuduc, Richard and Ying, Lexing and Zorin, Denis and Biros, George},
 title = {A Massively Parallel Adaptive Fast-multipole Method on Heterogeneous Architectures},
 booktitle = {Proceedings of the Conference on High Performance Computing Networking, Storage and Analysis},
 series = {SC '09},
 year = {2009},
 isbn = {978-1-60558-744-8},
 location = {Portland, Oregon},
 pages = {58:1--58:12},
 articleno = {58},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1654059.1654118},
 doi = {10.1145/1654059.1654118},
 acmid = {1654118},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@article{YOKOTA20111272,
title = "Biomolecular electrostatics using a fast multipole BEM on up to 512 gpus and a billion unknowns",
journal = "Computer Physics Communications",
volume = "182",
number = "6",
pages = "1272 - 1283",
year = "2011",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2011.02.013",
url = "http://www.sciencedirect.com/science/article/pii/S0010465511000750",
author = "Rio Yokota and Jaydeep P. Bardhan and Matthew G. Knepley and L.A. Barba and Tsuyoshi Hamada",
keywords = "Bioelectrostatics, Fast multipole method, Boundary element method, Integral equations, Graphics processors, "
}

@article{YOKOTA20092066,
title = "Fast multipole methods on a cluster of GPUs for the meshless simulation of turbulence",
journal = "Computer Physics Communications",
volume = "180",
number = "11",
pages = "2066 - 2078",
year = "2009",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2009.06.009",
url = "http://www.sciencedirect.com/science/article/pii/S0010465509001891",
author = "R. Yokota and T. Narumi and R. Sakamaki and S. Kameoka and S. Obi and K. Yasuoka",
keywords = "Fast multipole method, Pseudo-particle method, Graphics processing unit, Particle method"
}

@ARTICLE{6553090,
author={Q. M. Nguyen and V. Dang and O. Kilic and E. El-Araby},
journal={IEEE Antennas and Wireless Propagation Letters},
title={Parallelizing Fast Multipole Method for Large-Scale Electromagnetic Problems Using GPU Clusters},
year={2013},
volume={12},
number={},
pages={868-871},
keywords={computational electromagnetics;electromagnetic compatibility;graphics processing units;GPU clusters;Nvidia Tesla M2090 GPU;computational capability;graphics processing unit;large-scale electromagnetic problems;parallelizing fast multipole method;Fast Multipole Method (FMM);graphics processing unit (GPU);high-performance clusters;iterative solvers;method of moments (MoM)},
doi={10.1109/LAWP.2013.2271743},
ISSN={1536-1225},
month={},}

@INPROCEEDINGS{5470415,
author={A. Chandramowlishwaran and S. Williams and L. Oliker and I. Lashuk and G. Biros and R. Vuduc},
booktitle={2010 IEEE International Symposium on Parallel Distributed Processing (IPDPS)},
title={Optimizing and tuning the fast multipole method for state-of-the-art multicore architectures},
year={2010},
volume={},
number={},
pages={1-12},
keywords={multiprocessing systems;optimisation;GPU architecture;Nehalem;OpenMP parallelization;algorithmic tuning;data structure transformations;double-precision performance;fast multipole method;low-level tuning;multicore architectures;numerical approximation;single-node performance optimization;Approximation algorithms;Computer architecture;Data structures;Educational institutions;Kernel;Laboratories;Multicore processing;Optimization methods;Performance analysis;Sun},
doi={10.1109/IPDPS.2010.5470415},
ISSN={1530-2075},
month={April},}

@article{GUMEROV20088290,
title = "Fast multipole methods on graphics processors",
journal = "Journal of Computational Physics",
volume = "227",
number = "18",
pages = "8290 - 8313",
year = "2008",
issn = "0021-9991",
doi = "https://doi.org/10.1016/j.jcp.2008.05.023",
url = "http://www.sciencedirect.com/science/article/pii/S0021999108002921",
author = "Nail A. Gumerov and Ramani Duraiswami"
}

@article{doi:10.1177/1094342011429952,
author = {Rio Yokota and Lorena A Barba},
title ={A tuned and scalable fast multipole method as a preeminent algorithm for exascale systems},
journal = {The International Journal of High Performance Computing Applications},
volume = {26},
number = {4},
pages = {337-346},
year = {2012},
doi = {10.1177/1094342011429952},

URL = { 
        https://doi.org/10.1177/1094342011429952
    
},
eprint = { 
        https://doi.org/10.1177/1094342011429952
    
}
,
    abstract = { Among the algorithms that are likely to play a major role in future exascale computing, the fast multipole method (fmm) appears as a rising star. Our previous recent work showed scaling of an fmm on gpu clusters, with problem sizes of the order of billions of unknowns. That work led to an extremely parallel fmm, scaling to thousands of gpus or tens of thousands of cpus. This paper reports on a campaign of performance tuning and scalability studies using multi-core cpus, on the Kraken supercomputer. All kernels in the fmm were parallelized using OpenMP, and a test using 107 particles randomly distributed in a cube showed 78\% efficiency on 8 threads. Tuning of the particle-to-particle kernel using single instruction multiple data (SIMD) instructions resulted in 4 × speed-up of the overall algorithm on single-core tests with 103–107 particles. Parallel scalability was studied in both strong and weak scaling. The strong scaling test used 108 particles and resulted in 93\% parallel efficiency on 2048 processes for the non-SIMD code and 54\% for the SIMD-optimized code (which was still 2 × faster). The weak scaling test used 106 particles per process, and resulted in 72\% efficiency on 32,768 processes, with the largest calculation taking about 40 seconds to evaluate more than 32 billion unknowns. This work builds up evidence for our view that fmm is poised to play a leading role in exascale computing, and we end the paper with a discussion of the features that make it a particularly favorable algorithm for the emerging heterogeneous and massively parallel architectural landscape. The code is open for unrestricted use under the MIT license. }
}

@inproceedings{Choi:2014:CGH:2588768.2576787,
 author = {Choi, Jee and Chandramowlishwaran, Aparna and Madduri, Kamesh and Vuduc, Richard},
 title = {A CPU: GPU Hybrid Implementation and Model-Driven Scheduling of the Fast Multipole Method},
 booktitle = {Proceedings of Workshop on General Purpose Processing Using GPUs},
 series = {GPGPU-7},
 year = {2014},
 isbn = {978-1-4503-2766-4},
 location = {Salt Lake City, UT, USA},
 pages = {64:64--64:71},
 articleno = {64},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/2576779.2576787},
 doi = {10.1145/2576779.2576787},
 acmid = {2576787},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {GPU, exascale, fast multipole method, hybrid, multicore, performance model},
}

@inproceedings{Chandramowlishwaran:2012:BAT:2312005.2312039,
 author = {Chandramowlishwaran, Aparna and Choi, JeeWhan and Madduri, Kamesh and Vuduc, Richard},
 title = {Brief Announcement: Towards a Communication Optimal Fast Multipole Method and Its Implications at Exascale},
 booktitle = {Proceedings of the Twenty-fourth Annual ACM Symposium on Parallelism in Algorithms and Architectures},
 series = {SPAA '12},
 year = {2012},
 isbn = {978-1-4503-1213-4},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {182--184},
 numpages = {3},
 url = {http://doi.acm.org/10.1145/2312005.2312039},
 doi = {10.1145/2312005.2312039},
 acmid = {2312039},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache complexity analysis, exascale, fast multipole method, performance modeling},
}

@INPROCEEDINGS{,
author = {A. Ahmadia and R. Yokota and E. Yunis},
booktitle = {2012 11th International Symposium on Parallel and Distributed Computing (ISPDC 2012)(ISPDC)},
title = {Scalable Force Directed Graph Layout Algorithms Using Fast Multipole Methods},
year = {2012},
volume = {00},
number = {},
pages = {180-187},
keywords={Layout;Springs;Runtime;Graphics processing unit;Force;Libraries;Electrostatics;Multi-GPUs;Force directed graph layout;Fast multipole methods},
doi = {10.1109/ISPDC.2012.32},
url = {doi.ieeecomputersociety.org/10.1109/ISPDC.2012.32},
ISSN = {},
month={06}
}

@INPROCEEDINGS{7089024,
author={B. Zhang},
booktitle={2014 Fourth Workshop on Data-Flow Execution Models for Extreme Scale Computing},
title={Asynchronous Task Scheduling of the Fast Multipole Method Using Various Runtime Systems},
year={2014},
volume={},
number={},
pages={9-16},
keywords={parallel algorithms;scheduling;C++11 standard thread;Cilk;HPX-5 library;OpenMP task;adaptive fast multipole method;asynchronous task scheduling;eager thread creation;high performance ParalleX library;lazy thread creation;runtime system;Accuracy;Adaptation models;Computational modeling;Instruction sets;Libraries;Processor scheduling;Runtime;C++11;Cilk;Data-driven;Fast Multipole Method;HPX-5;OpenMP},
doi={10.1109/DFM.2014.14},
ISSN={},
month={Aug},}

@article{DBLP:journals/corr/abs-1206-0115,
  author    = {Emmanuel Agullo and
               B{\'{e}}renger Bramas and
               Olivier Coulaud and
               Eric Darve and
               Matthias Messner and
               Toru Takahashi},
  title     = {Pipelining the Fast Multipole Method over a Runtime System},
  journal   = {CoRR},
  volume    = {abs/1206.0115},
  year      = {2012},
  url       = {http://arxiv.org/abs/1206.0115},
  archivePrefix = {arXiv},
  eprint    = {1206.0115},
  timestamp = {Wed, 07 Jun 2017 14:41:34 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1206-0115},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{7912335,
author={E. Agullo and O. Aumage and B. Bramas and O. Coulaud and S. Pitoiset},
journal={IEEE Transactions on Parallel and Distributed Systems},
title={Bridging the Gap Between OpenMP and Task-Based Runtime Systems for the Fast Multipole Method},
year={2017},
volume={28},
number={10},
pages={2794-2807},
keywords={application program interfaces;multiprocessing systems;parallel programming;program diagnostics;HPC numerical codes;OpenMP specification;ScalFMM;application program interface;fast multipole method;high performance computing;high-performance numerical library;multicore processors;parallel programming;programming tractability;shared-memory platforms;task-based runtime systems;Computational modeling;Libraries;Multicore processing;Parallel processing;Program processors;Programming;Runtime;High performance computing;OpenMP;commutativity;compiler;fast multipole method;multicore architecture;parallel programming model;priority;runtime system},
doi={10.1109/TPDS.2017.2697857},
ISSN={1045-9219},
month={Oct},}

@article{doi:10.1137/130915662,
author = {Agullo, E. and Bramas, B. and Coulaud, O. and Darve, E. and Messner, M. and Takahashi, T.},
title = {Task-Based FMM for Multicore Architectures},
journal = {SIAM Journal on Scientific Computing},
volume = {36},
number = {1},
pages = {C66-C93},
year = {2014},
doi = {10.1137/130915662},

URL = { 
        https://doi.org/10.1137/130915662
    
},
eprint = { 
        https://doi.org/10.1137/130915662
    
}

}

@INPROCEEDINGS{7823860,
author={K. Fukuda and M. Matsuda and N. Maruyama and R. Yokota and K. Taura and S. Matsuoka},
booktitle={2016 IEEE 22nd International Conference on Parallel and Distributed Systems (ICPADS)},
title={Tapas: An Implicitly Parallel Programming Framework for Hierarchical N-Body Algorithms},
year={2016},
volume={},
number={},
pages={1100-1109},
keywords={C++ language;distributed processing;graphics processing units;parallel programming;C++ programming framework;C++ template metaprogramming;Tapas;distributed memory strong scaling evaluation;distributed nodes;fast multipole method;heterogeneous multicore;hierarchical N-body algorithms;implicitly parallel programming framework;inspector executor style code;irregular data access;multiGPU version;multinode environment;parallel program;prototype implementation;scientific applications;Algorithm design and analysis;Approximation algorithms;C++ languages;Force;Libraries;Programming;Standards;Fast Multipole Method;GPGPU;Hierarchical algorithms;N-body algorithms;Programming Framework},
doi={10.1109/ICPADS.2016.0145},
ISSN={1521-9097},
month={Dec},}

@article{doi:10.1002/cpe.3132,
author = {Ltaief Hatem and Yokota Rio},
title = {Data-driven execution of fast multipole methods},
journal = {Concurrency and Computation: Practice and Experience},
volume = {26},
number = {11},
pages = {1935-1946},
keywords = {fast multipole methods, load balancing, dynamic scheduling},
doi = {10.1002/cpe.3132},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.3132},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.3132},
abstract = {SUMMARYFast multipole methods (FMMs) have complexity, are compute bound, and require very little synchronization, which makes them a favorable algorithm on next-generation supercomputers. Their most common application is to accelerate N-body problems, but they can also be used to solve boundary integral equations. When the particle distribution is irregular and the tree structure is adaptive, load balancing becomes a non-trivial question. A common strategy for load balancing FMMs is to use the work load from the previous step as weights to statically repartition the next step. The authors discuss in the paper another approach based on data-driven execution to efficiently tackle this challenging load balancing problem. The core idea consists of breaking the most time-consuming stages of the FMMs into smaller tasks. The algorithm can then be represented as a directed acyclic graph where nodes represent tasks and edges represent dependencies among them. The execution of the algorithm is performed by asynchronously scheduling the tasks using the queueing and runtime for kernels runtime environment, in a way such that data dependencies are not violated for numerical correctness purposes. This asynchronous scheduling results in an out-of-order execution. The performance results of the data-driven FMM execution outperform the previous strategy and show linear speedup on a quad-socket quad-core Intel Xeon system.Copyright © 2013 John Wiley \& Sons, Ltd.}
}

@InProceedings{10.1007/978-3-642-38750-0_19,
author="Amer, Abdelhalim
and Maruyama, Naoya
and Peric{\`a}s, Miquel
and Taura, Kenjiro
and Yokota, Rio
and Matsuoka, Satoshi",
editor="Kunkel, Julian Martin
and Ludwig, Thomas
and Meuer, Hans Werner",
title="Fork-Join and Data-Driven Execution Models on Multi-core Architectures: Case Study of the FMM",
booktitle="Supercomputing",
year="2013",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="255--266",
abstract="Extracting maximum performance of multi-core architectures is a difficult task primarily due to bandwidth limitations of the memory subsystem and its complex hierarchy. In this work, we study the implications of fork-join and data-driven execution models on this type of architecture at the level of task parallelism. For this purpose, we use a highly optimized fork-join based implementation of the FMM and extend it to a data-driven implementation using a distributed task scheduling approach. This study exposes some limitations of the conventional fork-join implementation in terms of synchronization overheads. We find that these are not negligible and their elimination by the data-driven method, with a careful data locality strategy, was beneficial. Experimental evaluation of both methods on state-of-the-art multi-socket multi-core architectures showed up to 22{\%} speed-ups of the data-driven approach compared to the original method. We demonstrate that a data-driven execution of FMM not only improves performance by avoiding global synchronization overheads but also reduces the memory-bandwidth pressure caused by memory-intensive computations.",
isbn="978-3-642-38750-0"
}

@article{Kurzak:2005:MPI:1088519.1088526,
 author = {Kurzak, Jakub and Pettitt, B. Montgomery},
 title = {Massively Parallel Implementation of a Fast Multipole Method for Distributed Memory Machines},
 journal = {J. Parallel Distrib. Comput.},
 issue_date = {July 2005},
 volume = {65},
 number = {7},
 month = jul,
 year = {2005},
 issn = {0743-7315},
 pages = {870--881},
 numpages = {12},
 url = {http://dx.doi.org/10.1016/j.jpdc.2005.02.001},
 doi = {10.1016/j.jpdc.2005.02.001},
 acmid = {1088526},
 publisher = {Academic Press, Inc.},
 address = {Orlando, FL, USA},
 keywords = {Molecular dynamics algorithms},
} 

@INPROCEEDINGS{1592917,
author={Lexing Ying and G. Biros and D. Zorin and H. Langston},
booktitle={Supercomputing, 2003 ACM/IEEE Conference},
title={A New Parallel Kernel-Independent Fast Multipole Method},
year={2003},
volume={},
number={},
pages={14-14},
keywords={Fast multipole methods;N-body problems;adaptive algorithms;boundary integral;equations;massively parallel computing;viscous flows;Acceleration;Adaptive algorithm;Algorithm design and analysis;Concurrent computing;Integral equations;Kernel;Laplace equations;Parallel processing;Permission;Scalability;Fast multipole methods;N-body problems;adaptive algorithms;boundary integral;equations;massively parallel computing;viscous flows},
doi={10.1109/SC.2003.10013},
ISSN={},
month={Nov},}

@article{debuhr_zhang_tsueda_tilstra-smith_sterling_2016,
title={DASHMM: Dynamic Adaptive System for Hierarchical Multipole Methods},
volume={20},
DOI={10.4208/cicp.030316.310716sw},
number={4},
journal={Communications in Computational Physics},
publisher={Cambridge University Press},
author={DeBuhr, J. and Zhang, B. and Tsueda, A. and Tilstra-Smith, V. and Sterling, T.},
year={2016},
pages={1106–1126}}

@article{dashmm-rev,
author={DeBuhr, J. and Zhang, B. and Sterling, T.},
title={Revision of DASHMM: Dynamic Adaptive System for Hierarchical Multipole Methods},
volume={23},
number={1},
doi={10.4208/cicp.230517.300617sw},
journal={Communications in Computational Physics},
publisher={Cambridge University Press},
year={2018},
pages={296-314}}

@article{dashmm-ipdps,
author={DeBuhr, J. and Zhang, B. and D'Alessandro, L.},
title={Scalable Hierarchical Multipole Methods using an Asynchronous Many-Tasking Runtime System},
booktile={2017 IEEE International Parallel and Distributed Processing Symposium Workshops},
doi={10.1109/IPDPSW.2017.88},
pages={1226-1234},
year={2017}}

